#!/usr/bin/env python
# coding: utf-8

# In[ ]:


# # -*- coding: utf-8 -*-
# """DM-Project.ipynb

# Automatically generated by Colaboratory.

# Original file is located at
#     https://colab.research.google.com/drive/1yt9q6lUjBJTXwbBBFFuBk4BImwYzF-Cd
# """

# from google.colab import drive
# drive.mount('/content/drive')

import warnings
warnings.filterwarnings('ignore')

import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.model_selection import train_test_split
from sklearn.feature_selection import RFE
import sklearn.metrics
from sklearn.cluster import DBSCAN

from sklearn.metrics import confusion_matrix, precision_recall_fscore_support, precision_recall_curve, accuracy_score, recall_score
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier,NearestNeighbors
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, BaggingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC


# In[ ]:


#Reading the Train data
data=pd.read_json(r'C:\Users\parisa\Desktop\Uni\Data Mining\Project\Train Data.json', lines=True)
data.head(3)
data.shape
#(11099, 31)
data.columns
data.dtypes
#An example for detecting unique values and their frequency in each feature, like "source"
data['source'].value_counts()

#Detecting dictionaries in the dataset.
[i for i in data.columns if isinstance(data[i][0],dict)]
#['entities', 'metadata', 'user', 'retweeted_status']

data_info=pd.DataFrame(index=data.columns)  #Creating an empty dataframe
#Detecting missing values
data_info['# of NaNs'], data_info['an instance']=[data[i].isna().sum() for i in data.columns], [data[i].iloc[0] for i in data.columns]
data_info.sort_values(['# of NaNs'], ascending=True)


# In[13]:


#Cleaning the tweet's text
def clean_tweet(tweet):
    tweet=re.sub(r'\ART', ' ', tweet) # Omitting RT at the beginning
    tweet=re.sub(r'@\w+:*', ' ', tweet, count=1) # Omitting the first mention
    tweet=re.sub(r' +', ' ', tweet) # Merging consecutive spaces
    tweet=re.sub(r'\A +', '', tweet) # Omitting space at the beginning, if it exists
    return tweet

#Extracting useful features and columns from the dataset
def clean_dataset(data, train=True):
    new_data=pd.DataFrame(index=data.index)

    # media 
    new_data['is_media']=[int('media' in i.keys()) for i in data['entities']] 
    # truncated
    new_data['truncated']=[int(i) for i in data['truncated']]
    # tweet_length
    new_data['tweet-length']=[len(clean_tweet(i)) for i in data['text']]
    # tweet's posting time (second)
    new_data['tweet-post_time']=[i.second for i in data['created_at']]
    # number of hashtags in a tweet
    new_data['tweet-hashtags']=[len(i['hashtags']) for i in data['entities']]
    # number of symbols in a tweet
    new_data['tweet-symbols']=[len(i['symbols']) for i in data['entities']]
    # number of user-mentions in a tweet
    new_data['tweet-mentions']=[len(i['user_mentions']) for i in data['entities']]
    # number of urls in a tweet
    new_data['tweet-urls']=[len(i['urls']) for i in data['entities']]
    #tweet is quote
    new_data['tweet_is_quote']=[int(i) for i in data['is_quote_status']]
    # user has description
    new_data['user-has_description']=[int(len(i['description'])>0) for i in data['user']]
    # user's followers' count
    new_data['user-followers_count']=[i['followers_count'] for i in data['user']]
    # user's friends' count
    new_data['user-friends_count']=[i['friends_count'] for i in data['user']]
    # user's listed count
    new_data['user-listed_count']=[i['listed_count'] for i in data['user']]
    # user's favourites' count
    new_data['user-favourites_count']=[i['favourites_count'] for i in data['user']]
    # user's statuses' count
    new_data['user-statuses_count']=[i['statuses_count'] for i in data['user']]
    # the year user created his account at
    new_data['user-created_at']=[re.findall(r'[0-9]+', i['created_at'])[-1] for i in data['user']]
    
    if(train):
        # viral
        new_data['viral']=[int(i) for i in data['retweet_count']>(data['retweet_count'].median())]  #defining viral tweets based
        #on retweet count's median in the train dataset
    
    return new_datab


# In[36]:


#train data
train_data=clean_dataset(data)
train_data.head(5)

#Correlation
sb.heatmap(train_data.corr())
plt.show()


# In[46]:


#eps optimization
neigh = NearestNeighbors(n_neighbors=2)
nbrs = neigh.fit(train_data)
distances, indices = nbrs.kneighbors(train_data)
distances = np.sort(distances, axis=0)
distances = distances[10000:11000,1]
plt.plot(np.arange(10000,11000), distances, color='gold')
plt.plot(np.arange(10000,11000)[800],distances[800],'bx')
plt.show()


# In[38]:


#outlier detection and removing using DBSCAN:
clustering = DBSCAN(eps=10800, min_samples=3).fit(train_data)
predict = clustering.labels_
out_count = list(predict).count(-1)
out_percentage = out_count/(train_data.shape[0])
X = train_data[~np.where(predict == -1, True, False)] #The train data after removing outliers
out_percentage


# In[39]:


#Setting x and y for our dataset
x = X.drop(['viral'], axis=1, inplace=False)
y = list(X['viral'])
rep=3


# In[ ]:


#plots and visualization
#length
length = train_data['tweet-length']
length = pd.cut(length, bins=np.arange(0, 151, 30), labels=[0,1,2,3,4])
viral = np.array([])
not_viral = np.array([])
for i in np.arange(0,5,1):
    tmp = np.array(train_data['viral'])[np.where(length==i)]
    viral = np.append(viral, (tmp==1).sum())
    not_viral = np.append(not_viral, (tmp==0).sum())

# plt.figure(figsize=(9,6), dpi=100)
barwidth = 0.25
p1 = np.arange(len(viral))
p2 = [x + barwidth for x in p1]
plt.bar(p1, viral, barwidth, label='Viral',color = 'purple')
plt.bar(p2, not_viral, barwidth, label='not Viral',color = 'bisque')
plt.xlabel('Tweet Length')
plt.ylabel('Number of Tweets')
plt.xticks([r + barwidth/2 for r in np.arange(len(viral))], ['0-30', '31-60', '61-90', '91-120', '121-150'])
plt.legend()
plt.show()


# In[ ]:


#URL related plot
viral = np.array([])
not_viral = np.array([])
for i in np.arange(0,4,1):
    tmp = np.array(train_data['viral'])[np.where(train_data['tweet-urls']==i)]
    viral = np.append(viral, (tmp==1).sum())
    not_viral = np.append(not_viral, (tmp==0).sum())

# plt.figure(figsize=(9,6), dpi=100)
barwidth = 0.25
p1 = np.arange(len(viral))
p2 = [x + barwidth for x in p1]
plt.bar(p1, viral, barwidth, label='Viral',color = 'purple')
plt.bar(p2, not_viral, barwidth, label='not Viral',color = 'bisque')
plt.xlabel('Number of URLs')
plt.ylabel('Number of Tweets')
plt.xticks([r + barwidth/2 for r in np.arange(len(viral))], ['0', '1', '2', '3'])
plt.legend()
plt.show()


# In[3]:


#RFE is a backward selection method and it is used to select the best features among all features
# n: number of replications
def RFE_function(classifier, n, x=x, y=y):    
    features = np.array([])
    result = []
    for i in range(x.shape[1]):
        for j in range(n):
            x_train, x_test, y_train, y_test = train_test_split(x, y, stratify=y, test_size=0.2)
            acc = np.array([])
            recall = np.array([])
            selector = RFE(classifier, n_features_to_select=i+1, step=1)
            selector.fit(x_train,y_train)
            y_pred = selector.predict(x_test)
            acc = np.append(acc, sklearn.metrics.accuracy_score(y_test, y_pred)*100)
            recall = np.append(recall, sklearn.metrics.recall_score(y_test, y_pred)*100)
        #mean of replications
        result.append([acc.mean(), recall.mean()])
        features = np.append(features, selector.support_) # support of the rep: j=n-1
    result = pd.DataFrame(result, columns=['Accuracy', 'Recall'])
    result.insert(0, "Number of Features", np.arange(1,x.shape[1]+1,1))
    features = features.reshape(-1,x_train.shape[1])
    features = pd.DataFrame(features, columns=x.columns)
    features.insert(0, "Number of Features", np.arange(1,x.shape[1]+1,1))
    return result, features


# In[79]:


# LogisticRegression
res=[]
for penalty in ['l1', 'l2']:
    rfe_result, rfe_features=RFE_function(LogisticRegression(penalty=penalty), n=3)
    tmp_df=rfe_result.sort_values(by=['Accuracy', 'Recall'], ascending=False)
    avg_accuracy=tmp_df['Accuracy'].iloc[0]
    avg_recall=tmp_df['Recall'].iloc[0]
    index=tmp_df.index[0]
    selected_features=rfe_features.iloc[index]
    res.append([{'penalty':penalty}, [selected_features], avg_accuracy, avg_recall])
LogisticRegression_df=pd.DataFrame(data=res, columns=['tuned_arguments', 'selected_features','avg. accuracy', 'avg. recall'])
#saving dataframe as csv
LogisticRegression_df.to_csv(r'C:\Users\parisa\Desktop\Uni\Data Mining\Project\LogisticRegression_df.csv')


# In[80]:


# random forest
res=[]
for n in [10, 30]:
    for cr in ['gini', 'entropy']:
        for dp in [5, 15]:
            for sp in np.arange(5,21,15):
                for sl in [2,10]:
                    rfe_result, rfe_features=RFE_function(RandomForestClassifier(n_estimators = n, criterion=cr, max_depth=dp, min_samples_split=sp, min_samples_leaf=sl), n=3)
                    tmp_df=rfe_result.sort_values(by=['Accuracy', 'Recall'], ascending=False)
                    avg_accuracy=tmp_df['Accuracy'].iloc[0]
                    avg_recall=tmp_df['Recall'].iloc[0]
                    index=tmp_df.index[0]
                    selected_features=rfe_features.iloc[index]
                    res.append([{'n_estimators': n, 'criterion': cr, 'max_depth': dp, 'min_samples_split': sp, 'min_samples_leaf': sl}, [selected_features], avg_accuracy, avg_recall])
RandomForest_df=pd.DataFrame(data=res, columns=['tuned_arguments', 'selected_features','avg. accuracy', 'avg. recall'])
#saving dataframe as csv
RandomForest_df.to_csv(r'C:\Users\parisa\Desktop\Uni\Data Mining\Project\RandomForest_df.csv')


# In[125]:


bagging_features=RandomForest_df.sort_values(by=['avg. accuracy', 'avg. recall'], ascending=False, ignore_index=True)['selected_features'].iloc[0]

"""not selected features

tweet-mentions

user-has_description
"""

x_bagging=x.drop(columns=['tweet-mentions', 'user-has_description'])

# Bagging
res=[]
for bt in [True, False]:
  for n_est in [10, 20, 30, 40]:
    bagging=BaggingClassifier(n_estimators=n_est, bootstrap=bt)
    temp_acc, temp_rec=0, 0
    for n in range(rep):
      x_train, x_test, y_train, y_test = train_test_split(x_bagging, y, stratify=y, test_size=0.2)
      bagging.fit(x_train, y_train)
      predicted=bagging.predict(x_test)
      temp_acc+=accuracy_score(y_test, predicted)*100.0
      temp_rec+=recall_score(y_test, predicted)*100.0
    avg_accuracy=temp_acc/rep
    avg_recall=temp_rec/rep
    res.append([{'bootstrap':bt, 'n_estimator':n_est}, bagging_features, avg_accuracy, avg_recall])
Bagging_df=pd.DataFrame(data=res, columns=['tuned_arguments', 'selected_features', 'avg. accuracy', 'avg. recall'])

#saving dataframe as csv
Bagging_df.to_csv('/content/drive/My Drive/projects/DM-Project/Bagging_df.csv')


# In[155]:


"""Conclusion"""
#The "info" dataframe consists of the best results of each classifier with details such
#as parameters, features, etc ...
info=[]
#LogisticRegression
df=pd.read_csv(r'C:\Users\parisa\Desktop\Uni\Data Mining\Project\LogisticRegression_df.csv').sort_values(by=['avg. accuracy', 'avg. recall'], ascending=False, ignore_index=True)
info.append(['LogisticRegression', df['tuned_arguments'][0], df['selected_features'][0], df['avg. accuracy'][0], df['avg. recall'][0]])

#RandomForest
df=pd.read_csv(r'C:\Users\parisa\Desktop\Uni\Data Mining\Project\RandomForest_df.csv').sort_values(by=['avg. accuracy', 'avg. recall'], ascending=False, ignore_index=True)
info.append(['RandomForest', df['tuned_arguments'][0], df['selected_features'][0], df['avg. accuracy'][0], df['avg. recall'][0]])

#Bagging
df=pd.read_csv(r'C:\Users\parisa\Desktop\Uni\Data Mining\Project\Bagging_df.csv').sort_values(by=['avg. accuracy', 'avg. recall'], ascending=False, ignore_index=True)
info.append(['Bagging', df['tuned_arguments'][0], df['selected_features'][0], df['avg. accuracy'][0], df['avg. recall'][0]])

# the main DF containing main info.
info=pd.DataFrame(data=info, columns=['classifier', 'tuned_arguments', 'selected_features', 'Avg. Accuracy', 'Avg. Recall'])
info


# In[11]:


#predict test
test=pd.read_json(r'C:\Users\parisa\Desktop\Uni\Data Mining\Project\Test_Data.json', lines=True)


# In[12]:


x_test=clean_dataset(test, train=False)
x_test.drop(columns=['user-has_description','tweet-mentions'], inplace=True)
x_test.head()
x_train = x.drop(columns=['user-has_description','tweet-mentions'], inplace=False)
y_train = y
classifier = RandomForestClassifier(n_estimators = 30, criterion='gini', max_depth=15, min_samples_split=20, min_samples_leaf=2)
classifier.fit(x_train,y_train)
test_result = classifier.predict(x_test)
test['Viral_prediction']=test_result
test.to_csv(r'C:\Users\parisa\Desktop\Uni\Data Mining\Project\test_data.csv')


# In[5]:


x_train = x.drop(columns=['user-has_description','tweet-mentions'], inplace=False)
y_train = y
classifier = RandomForestClassifier(n_estimators = 30, criterion='gini', max_depth=15, min_samples_split=20, min_samples_leaf=2)
classifier.fit(x_train,y_train)


# In[10]:


for i in np.arange(0,31,5):
    tree.plot_tree(classifier.estimators_[i],
               feature_names = x_train.columns, 
               class_names=['0','1'],
               filled = True);
    fig.savefig('D:/darsi uni/terme8/data/proj/Project1/rf_individualtree'+ str(i)+'.png')


# In[ ]:




